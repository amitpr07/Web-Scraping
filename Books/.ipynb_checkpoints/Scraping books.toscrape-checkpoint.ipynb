{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c54572",
   "metadata": {},
   "source": [
    "# Scraping books.toscrape.com\n",
    "\n",
    "1. Web scraping is the process of using tools to extract content and data from a website\n",
    "\n",
    "2. The tools used in the project are Python programming language and the various libraries available in python namely:\n",
    "    - requests (to get webapge in http format) https://docs.python-requests.org/en/latest/\n",
    "    - Beautiful Soup 4 (to convert the html data into parsable form) https://beautiful-soup-4.readthedocs.io/en/latest/\n",
    "    - Pandas (to create a database of the scraped information and make a csv file) https://pandas.pydata.org/\n",
    "    - Regular Expressions 're' (makes dealing with expression and text easier) https://docs.python.org/3/library/re.html\n",
    "    - os (to use operating system based functions in our programs) https://docs.python.org/3/library/os.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b4358",
   "metadata": {},
   "source": [
    "## Task Statement\n",
    "\n",
    "  - To scrape a 'https://books.toscrape.com' for the most common quote_tags and to get top 10 quotes from each tag\n",
    "  \n",
    "  - 'books.toscrape.com' is a free web scraping sandbox to help people practise web-scraping\n",
    "  \n",
    "  - We will output a CSV file containg the first 20 books and the links to each book\n",
    "  \n",
    "  - Then for each book we will output a CSV file containg UPC, Name, Genre, Stars, Price, Availability and Description which we will store in a separate folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7fc9d",
   "metadata": {},
   "source": [
    "### The format for CSV file :\n",
    "\n",
    "   - for the Quotes.csv:\n",
    "\n",
    "        Name, URL\n",
    "        \n",
    "\n",
    "   - for the CSV files for each tag:\n",
    "   \n",
    "        UPC, Name, Genre, Stars, Price, Availability, Description\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea753877",
   "metadata": {},
   "source": [
    "#### Scraping the homepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59d0a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_books(page_url):\n",
    "    \n",
    "    #get the page\n",
    "    response=requests.get(page_url)\n",
    "    \n",
    "    #check the status of response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load home page {}'.format(page_url))\n",
    "    \n",
    "    #convert into BS4 for parsing\n",
    "    homepage_doc=BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    return homepage_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcd76e4",
   "metadata": {},
   "source": [
    "returns the homepage in parsable form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb2d18a",
   "metadata": {},
   "source": [
    "#### Getting the name of the top 20 books and their URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dfe11a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bookurl(homepage_doc):\n",
    "    \n",
    "    #initializing lists\n",
    "    book_names=[]\n",
    "    book_webpage=[]\n",
    "    \n",
    "    #get tags of the books\n",
    "    ttags=homepage_doc.find_all('h3')\n",
    "    #get all the books' name and url\n",
    "    for i in range(0,20):\n",
    "        book_names.append(ttags[i].a['title'])\n",
    "        book_webpage.append('https://books.toscrape.com/'+ttags[i].a['href'])\n",
    "        \n",
    "    return [book_names,book_webpage]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e33cd4f",
   "metadata": {},
   "source": [
    "returns a list of book names and the urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ecc69e",
   "metadata": {},
   "source": [
    "#### Getting the url for each book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62cdf2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_details(book_url):\n",
    "    \n",
    "    #get the page\n",
    "    res=requests.get(book_url)\n",
    "    \n",
    "    #check the status of response\n",
    "    if res.status_code != 200:\n",
    "        raise Exception('Failed to load page {}.format(book_url)')\n",
    "    \n",
    "    #convert into BS4 for parsing\n",
    "    book_page_doc=BeautifulSoup(res.text,'html.parser')\n",
    "    \n",
    "    return book_page_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50e7653",
   "metadata": {},
   "source": [
    "returns the book page in parsible form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a81928",
   "metadata": {},
   "source": [
    "#### Getting the book details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8df88e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_details(page_doc):\n",
    "    \n",
    "    # get the name, stars, price and availability\n",
    "    tag=page_doc.find_all('div',{'class':\"col-sm-6 product_main\"})\n",
    "    Name=tag[0].h1.text.strip()\n",
    "    Stars=tag[0].find_all('p')[2]['class'][1]\n",
    "    Price=tag[0].find('p',{'class':'price_color'}).text[1:]\n",
    "    Availability=tag[0].find_all('p',{'class':'instock'})[0].text.strip()\n",
    "    \n",
    "    #get genre\n",
    "    gen=page_doc.find_all('ul',{'class':'breadcrumb'})\n",
    "    Genre=gen[0].find_all('a')[2].text.strip()\n",
    "    \n",
    "    #get description\n",
    "    p_tags=page_doc.find_all('p')\n",
    "    Description=p_tags[3].text.strip()\n",
    "    \n",
    "    #get UPC\n",
    "    details=page_doc.find_all('table',{'class':'table table-striped'})\n",
    "    UPC=details[0].find_all('tr')[0].text.strip()[3:]\n",
    "    \n",
    "    return [UPC,Name,Genre,Stars,Price,Availability,Description]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9402f",
   "metadata": {},
   "source": [
    "returns the book details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82a3cc4",
   "metadata": {},
   "source": [
    "#### Writing all the details into CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3919618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(url):\n",
    "\n",
    "    #get the parser page\n",
    "    doc_parse=get_page_details(url)\n",
    "    \n",
    "    #get all the details in UPC, Name, Genre, Stars, Price, Availability, Description order\n",
    "    detail_list=get_book_details(doc_parse)\n",
    "    \n",
    "    #to store the name of the book by\n",
    "    #removing the ':' in the name as it causes error in writingaa to csv file\n",
    "    N=' '.join(re.findall(r\"[^: ]+\",detail_list[1]))\n",
    "    \n",
    "    #creating directory to store data\n",
    "    os.makedirs('Data',exist_ok=True)\n",
    "    \n",
    "    #checking if file already exists in case of failures\n",
    "    fname='Data'+'/'+N+'.csv'\n",
    "    if os.path.exists(fname):\n",
    "        print(\"The file {} already exists....Skipping\".format(fname))\n",
    "        return\n",
    "    \n",
    "    #writing to CSV\n",
    "    fto=open(fname,mode='w',newline='',encoding='UTF-8')\n",
    "    csv_writer=csv.writer(fto,delimiter=',')\n",
    "    csv_writer.writerows([['UPC', 'Name', 'Genre', 'Stars', 'Price', 'Availability', 'Description'],[detail_list[0],detail_list[1],detail_list[2],detail_list[3],detail_list[4],detail_list[5],detail_list[6]]])\n",
    "    #closing the CSV\n",
    "    fto.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f47fc",
   "metadata": {},
   "source": [
    "writes all the informatiom about each book to csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665456e",
   "metadata": {},
   "source": [
    "#### The driver function to run the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4b50199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "def driver():\n",
    "    \n",
    "    url='https://books.toscrape.com/'\n",
    "    i=scrape_books(url)\n",
    "    the_lists=get_bookurl(i)\n",
    "    \n",
    "    #create dataframe using pandas\n",
    "    dict1={'Name':the_lists[0],\"URL\":the_lists[1]}\n",
    "    df=pd.DataFrame(dict1,index=list(range(1,21)))\n",
    "    \n",
    "    #create csv\n",
    "    if os.path.exists('Books.csv'):\n",
    "        print(\"The file {} already exists.....Skipping\".format('Books.csv'))\n",
    "    else:\n",
    "        print(\"Creating Books.csv .....\")\n",
    "        df.to_csv('Books.csv',index=None)\n",
    "    \n",
    "    #Scraping the book webpages\n",
    "    for i in range(0,20):\n",
    "        print(\"Scraping {}\".format(the_lists[0][i]))\n",
    "        write_to_csv(the_lists[1][i])\n",
    "        \n",
    "    print(\"Done!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9250c8",
   "metadata": {},
   "source": [
    "Drives the code by creating Book.csv and other csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e27fce",
   "metadata": {},
   "source": [
    "#### Run the driver to begin scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5478508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Books.csv .....\n",
      "Scraping A Light in the Attic\n",
      "Scraping Tipping the Velvet\n",
      "Scraping Soumission\n",
      "Scraping Sharp Objects\n",
      "Scraping Sapiens: A Brief History of Humankind\n",
      "Scraping The Requiem Red\n",
      "Scraping The Dirty Little Secrets of Getting Your Dream Job\n",
      "Scraping The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull\n",
      "Scraping The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics\n",
      "Scraping The Black Maria\n",
      "Scraping Starving Hearts (Triangular Trade Trilogy, #1)\n",
      "Scraping Shakespeare's Sonnets\n",
      "Scraping Set Me Free\n",
      "Scraping Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\n",
      "Scraping Rip it Up and Start Again\n",
      "Scraping Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991\n",
      "Scraping Olio\n",
      "Scraping Mesaerion: The Best Science Fiction Stories 1800-1849\n",
      "Scraping Libertarianism for Beginners\n",
      "Scraping It's Only the Himalayas\n",
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64683216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
